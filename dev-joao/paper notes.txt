develop a robust methodology for quantifying semantic change
by evaluating word embeddings

1 - intro
changes in laguage are known to have some regularity
it's difficult to analyse these changes and their behavior
ex
	sound change
	semanthic change
central subject of interest: word frequency, polysemy
	changes as f(freq) and g(polysemy)???
need of a more scalable tool/method
proposal: embeddings from cooccurence-based embeddings
has been shown to be promising, but differente approches
novelty: broader work/more standard: 6 corpora, 200 years, 4 languages
propose 2 statistical laws: a) The law of conformity b) The law of innovation

2 - method
constructi embeddings in each time-period
aligning them over time
embeddings: ppmi, svd, word2vec (or sgns)
datasets: 6 datasets from Google N-Grams and COHA corpus
	select COHA!!!! 
		(American) English 
		Genre-balanced 
		sample 4.1 × 108 
		1810-2009
		contains pre-extracted word lemmas
	aggregated to the granularity of decades
	!carefulnes with sampling methodology --> 2000's discarded (diff method)
symmetric context windows of size 4 (on each side)
embedding size:
	sgns: 300
	svd: 300
ortoghonal procruste to align the years
they align one year to another (1, 2), (2, 3), ... , (t, t+1), ...
	>>> use generalized procruste analysis instead?
	>>> PA -> get less changed words as anchors -> PA?
	>>> read about proscrutes analysis...
	>>> how much of the changes are due just to the method itself??? >>> doing many folds and checking how much they vary???
Pair-wise similarity time-series
	for each year t, words i and j, compute cos-sim(w_i(t), w_j(t))
	for each pair of words i and j, extract a TS from above with (t)_1^T
	compute Spearman correlation of the TS with time (shape is not important)
	>>> other comparison approach?
Measuring semantic displacement
	compute the cos-sim of word i on t and t + dt
	or displacement between word i on t and t + dt

3 - evaluation
synchronic accuracy 
	MEN similarity task
	>>> look up for what this is

diachronic validity
	detecting known shifts (28)
		capture whether pairs of words moved closer or further apart in semantic space during a pre-determined time-period
		sgns - best on all english (ENGALL)
		svd - best on coha
	discovering shifts from data
		top 10 words that changed most
		from the 1900s to the 1990s
		with semantic displacement metric
		relative frequencies above 10−5 in both decades
		*obs: the analysis of the paper is very subjective and has passed through several experts
		>>> it could be better to not rely much on this type of criteria, since we don't have the same resources

SVD - best on similarity 
SGNS - best on analogy tasks 


>>> use a lin comb of ensemble/boost to improve those above?

>>> fit the linear model (section 4) to include some other factor?
	>>> measure of synonimity? (inverse of polysemy)

>>> learn word2vec with a single NN where the layer OHE -> embedding is shared 
>>> mesure de distance avec toute les combinaisons de dist (triangle)

word2vec
https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae
